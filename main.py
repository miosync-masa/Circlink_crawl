import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import requests
from bs4 import BeautifulSoup
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, Document, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from upstash_vector import Index
from datetime import datetime
import psycopg2
import uuid
import json
import re

# Load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

# Set API key via OpenAI client instance
client = OpenAI(api_key=openai_api_key)

upstash_vector_endpoint = os.getenv("UPSTASH_VECTOR_ENDPOINT")
upstash_vector_token = os.getenv("UPSTASH_VECTOR_TOKEN")

# Increase chunk size to avoid large metadata errors
Settings.text_splitter = SentenceSplitter(chunk_size=4096)

# Configure UpStash VectorDB
vector_index = Index(url=upstash_vector_endpoint, token=upstash_vector_token)
storage_context = StorageContext.from_defaults(vector_store=vector_index)

DATABASE_URL = os.getenv("DATABASE_URL")

# Connect to the database
try:
    conn = psycopg2.connect(DATABASE_URL)
    cursor = conn.cursor()
except Exception as e:
    st.error(f"Database connection failed: {e}")

# Initialize an empty VectorStoreIndex
llama_index = VectorStoreIndex([], storage_context=storage_context)

# ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½œæˆæ—¥æ™‚ã¨ã—ã¦è¨­å®š
created_at = datetime.now().strftime("%Y-%m-%d")

# Streamlit page configuration
st.set_page_config(page_title="Wantedly Job Registration", page_icon="ğŸ“„", layout="wide")
st.title("Wantedly Job Registration with LlamaIndex and UpStash")
st.write("Enter a Wantedly job URL and Job ID to fetch and save job data.")

# Initialize session state
if "job_data" not in st.session_state:
    st.session_state["job_data"] = None
if "tags" not in st.session_state:
    st.session_state["tags"] = None
if "job_id" not in st.session_state:
    st.session_state["job_id"] = None
if "metadata" not in st.session_state:
    st.session_state["metadata"] = None

# Input fields
job_url = st.text_input("Enter Wantedly Job URL")
job_id = st.text_input("Enter Job ID")

# ã‚¸ãƒ§ãƒ–IDæŠ½å‡ºé–¢æ•°
def extract_job_id_from_url(url):
    match = re.search(r'/(\d+)$', url)
    if match:
        return int(match.group(1))
    raise ValueError("Invalid URL format: Job ID not found")

# Wantedlyã‚¸ãƒ§ãƒ–è©³ç´°å–å¾—é–¢æ•°
def fetch_wantedly_job_details(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"Failed to fetch URL: {url}")

    soup = BeautifulSoup(response.text, "html.parser")
    meta_og_title = soup.find("meta", property="og:title")
    main_title = meta_og_title["content"] if meta_og_title else None

    company_name = None
    if main_title and "-" in main_title:
        parts = main_title.split("-")
        possible_company_text = parts[-2].strip()
        if "ã®æ¡ç”¨" in possible_company_text:
            company_name = possible_company_text.split("ã®æ¡ç”¨")[0].strip()
        else:
            company_name = possible_company_text

    sections = soup.find_all("section", class_="ProjectDescription__Section-sc-r2aril-9")
    what_text = why_text = how_text = do_text = None
    for sec in sections:
        title_el = sec.find("h3", class_="ProjectDescription__Title-sc-r2aril-1")
        if not title_el:
            continue
        heading = title_el.get_text(strip=True)
        content_div = sec.find("div", class_="ProjectDescription__DescriptionBase-sc-r2aril-8")
        content = content_div.get_text(strip=True) if content_div else ""
        if heading == "ãªã«ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹":
            what_text = content
        elif heading == "ãªãœã‚„ã‚‹ã®ã‹":
            why_text = content
        elif heading == "ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹":
            how_text = content
        elif heading == "ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™":
            do_text = content

    members = []
    member_sections = soup.find_all("section", class_="ProjectMemberListLaptop__Section-sc-11asydj-15")
    for ms in member_sections:
        name_tags = ms.find_all("p", class_="ProjectMemberListLaptop__ProjectMemberName-sc-11asydj-13")
        for nt in name_tags:
            member_name = nt.get_text(strip=True)
            members.append(member_name)

    stories = []
    heading_elems = soup.find_all("h3")
    for heading in heading_elems:
        if heading.get_text(strip=True) == "ä¼šç¤¾ã®æ³¨ç›®ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼":
            parent_section = heading.find_parent("section")
            if not parent_section:
                continue
            story_blocks = parent_section.find_all("div", class_="PinnedStoryList__Base-sc-w8czck-5")
            for block in story_blocks:
                title_box = block.find("h3", class_="PinnedStoryList__PostTitleBase-sc-w8czck-10")
                if not title_box:
                    continue
                link_tag = title_box.find("a")
                if not link_tag:
                    continue
                story_title = link_tag.get_text(strip=True)
                story_url = link_tag.get("href", "")
                stories.append({
                    "title": story_title,
                    "url": story_url,
                })

    return {
        "id": extract_job_id_from_url(url),
        "url": url,  # URLã‚’è¿½åŠ 
        "main_title": main_title,
        "company_name": company_name,
        "what": what_text,
        "why":  why_text,
        "how":  how_text,
        "do":   do_text,
        "members": members,
        "stories": stories,
    }

# Embeddingå–å¾—é–¢æ•°
def get_embedding(text, model="text-embedding-3-small"):
    # Call the embeddings endpoint via the client
    response = client.embeddings.create(
        input=text,
        model=model
    )
    return response.data[0].embedding

# ã‚¿ã‚°ç”Ÿæˆé–¢æ•°
def generate_tags(job_data):
    prompt = f"""
    ä»¥ä¸‹ã®æ±‚äººæƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¿ã‚°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:
    - ä¸»ãªå†…å®¹: {job_data['what']}
    - ãªãœã‚„ã‚‹ã®ã‹: {job_data['why']}
    - ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['how']}
    - ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™: {job_data['do']}

    å‡ºåŠ›ä¾‹: ["Python", "ãƒªãƒ¢ãƒ¼ãƒˆ", "ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ãƒ•ãƒ«ã‚¿ã‚¤ãƒ "]
    ã‚¿ã‚°ã¯JSONå½¢å¼ã®é…åˆ—ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """

    # Using the client to create ChatCompletion
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=8000,
        temperature=0.7
    )

    result_text = response.choices[0].message.content.strip()

    try:
        tags = json.loads(result_text)
        if isinstance(tags, list):
            return tags
        else:
            return [tags]
    except json.JSONDecodeError:
        return [result_text]

# ä¸€æ„ã®Llama IDç”Ÿæˆé–¢æ•°
def generate_llama_id():
    return str(uuid.uuid4()) 
 
# SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–¢æ•°        
def save_to_sql_db(job_data, sql_cursor):
    # LlamaIndexç”¨ã®ä¸€æ„IDã‚’ç”Ÿæˆ
    llama_id = generate_llama_id()

    # SQLã«ä¿å­˜
    sql_cursor.execute("""
        INSERT INTO job_metadata (id, llama_id, main_title, company_name, url, what, why, how, "do", created_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
    """, (
        job_data["id"],          # ã‚¸ãƒ§ãƒ–ID
        llama_id,                # LlamaIndexã®ä¸€æ„ID
        job_data["main_title"],
        job_data["company_name"],
        job_data["url"],
        job_data["what"],
        job_data["why"],
        job_data["how"],
        job_data["do"]
    ))

    return llama_id  # ä½œæˆã—ãŸLlamaIndexç”¨IDã‚’è¿”ã™    
    
def save_to_llama_index_with_embedding(job_data, index, sql_cursor):
    try:
        # SQLDBã«ä¿å­˜ã—ã€ä¸€æ„ã®Llama IDã‚’å–å¾—
        llama_id = save_to_sql_db(job_data, sql_cursor)

        # çµåˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        combined_text = f"""
        {job_data['main_title']}
        {job_data['company_name']}
        ãªã«ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['what']}
        ãªãœã‚„ã‚‹ã®ã‹: {job_data['why']}
        ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['how']}
        ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™: {job_data['do']}
        """

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedding = get_embedding(combined_text, model="text-embedding-3-small")
        tags = generate_tags(job_data)

        # Upstash Vectorã«ä¿å­˜
        upstash_data = {
            "id": llama_id,  # LlamaIndexç”¨ID
            "vector": embedding,  # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿
            "metadata": {
                "job_id": job_data["id"],
                "tags": tags,
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        }

        # å®Ÿéš›ã®ä¿å­˜å‡¦ç†
        index.upsert(vectors=[upstash_data])

        # çµæœã‚’è¾æ›¸å½¢å¼ã§è¿”ã™
        return {
            "llama_id": llama_id,
            "upstash_data": upstash_data,
            "status": "success",
            "message": "Data successfully saved to SQL and Upstash"
        }

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®ãƒ­ã‚°ã¨å¿œç­”
        return {
            "llama_id": None,
            "upstash_data": None,
            "status": "error",
            "message": f"Failed to save data: {e}"
        }

# --- Streamlit UI Flow ---
if st.button("Fetch Job Data"):
    # 1. å…¥åŠ›æ¤œè¨¼
    if not job_url:
        st.error("Please enter a URL.")
    elif not job_id:
        st.error("Please enter a valid Job ID.")
    else:
        try:
            # 2. Wantedlyã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            job_data = fetch_wantedly_job_details(job_url)
            
            # 3. LLMã‚’ä½¿ã£ãŸã‚¿ã‚°ç”Ÿæˆ
            tags = generate_tags(job_data)

            # 4. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ï¼ˆä¸€æ™‚ä¿å­˜ï¼‰
            st.session_state["job_data"] = job_data
            st.session_state["tags"] = tags
            st.session_state["job_id"] = job_id

            # 5. ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆHITLãƒ—ãƒ­ã‚»ã‚¹ï¼‰
            st.subheader("Fetched Job Data (Preview)")
            st.write("### Main Title:", job_data["main_title"])
            st.write("### Company Name:", job_data["company_name"])
            st.write("### What:", job_data["what"])
            st.write("### Why:", job_data["why"])
            st.write("### How:", job_data["how"])
            st.write("### Do:", job_data["do"])
            st.write("### Members:", job_data["members"])
            st.write("### Stories:", job_data["stories"])
            st.write("### Tags (Generated):", tags)

            st.info("Check the preview above. Then click 'Approve and Save' if it looks good.")
        except Exception as e:
            # ä¾‹å¤–å‡¦ç†
            st.error(f"Error fetching job data: {e}")

# 6. ä¿å­˜ï¼ˆHITLã®æ‰¿èªå¾Œã«å®Ÿè¡Œï¼‰
if st.session_state["job_data"] and st.session_state["job_id"] and st.button("Approve and Save"):
    try:
        # LlamaIndexã¨Upstash Vectorã¸ã®ä¿å­˜
        save_to_llama_index_with_embedding(st.session_state["job_data"], llama_index, cursor)

        st.success(
            f"Job '{st.session_state['job_data']['main_title']}' "
            f"with ID '{st.session_state['job_id']}' "
            "saved successfully to UpStash and SQL!"
        )
    except Exception as e:
        # ä¾‹å¤–å‡¦ç†
        st.error(f"Error saving job data: {e}")

