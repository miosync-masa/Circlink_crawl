import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import requests
from bs4 import BeautifulSoup
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, Document, StorageContext
from llama_index.core.node_parser import SentenceSplitter
from upstash_vector import Index
from datetime import datetime
import psycopg2
import uuid
import json
import re

# Load environment variables
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

# Set API key via OpenAI client instance
client = OpenAI(api_key=openai_api_key)

upstash_vector_endpoint = os.getenv("UPSTASH_VECTOR_ENDPOINT")
upstash_vector_token = os.getenv("UPSTASH_VECTOR_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")

# Increase chunk size to avoid large metadata errors
Settings.text_splitter = SentenceSplitter(chunk_size=4096)

# Configure UpStash VectorDB
vector_index = Index(url=upstash_vector_endpoint, token=upstash_vector_token)

# Connect to the database
try:
    conn = psycopg2.connect(DATABASE_URL)
    cursor = conn.cursor()
except Exception as e:
    st.error(f"Database connection failed: {e}")

# ç¾åœ¨ã®æ—¥ä»˜ã‚’ä½œæˆæ—¥æ™‚ã¨ã—ã¦è¨­å®š
created_at = datetime.now().strftime("%Y-%m-%d")

# Streamlit page configuration
st.set_page_config(page_title="Wantedly Job Registration", page_icon="ğŸ“„", layout="wide")
st.title("Wantedly Job Registration with LlamaIndex and UpStash")
st.write("Enter a Wantedly job URL and Job ID to fetch and save job data.")

# Initialize session state
if "job_data" not in st.session_state:
    st.session_state["job_data"] = None
if "tags" not in st.session_state:
    st.session_state["tags"] = None
if "job_id" not in st.session_state:
    st.session_state["job_id"] = None
if "metadata" not in st.session_state:
    st.session_state["metadata"] = None

# Input fields
job_url = st.text_input("Enter Wantedly Job URL")
job_id = st.text_input("Enter Job ID")

# ã‚¸ãƒ§ãƒ–IDæŠ½å‡ºé–¢æ•°
def extract_job_id_from_url(url):
    match = re.search(r'/(\d+)$', url)
    if match:
        return int(match.group(1))
    raise ValueError("Invalid URL format: Job ID not found")

# Wantedlyã‚¸ãƒ§ãƒ–è©³ç´°å–å¾—é–¢æ•°
def fetch_wantedly_job_details(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"Failed to fetch URL: {url}")

    soup = BeautifulSoup(response.text, "html.parser")
    meta_og_title = soup.find("meta", property="og:title")
    main_title = meta_og_title["content"] if meta_og_title else None

    company_name = None
    if main_title and "-" in main_title:
        parts = main_title.split("-")
        possible_company_text = parts[-2].strip()
        if "ã®æ¡ç”¨" in possible_company_text:
            company_name = possible_company_text.split("ã®æ¡ç”¨")[0].strip()
        else:
            company_name = possible_company_text

    sections = soup.find_all("section", class_="ProjectDescription__Section-sc-r2aril-9")
    what_text = why_text = how_text = do_text = None
    for sec in sections:
        title_el = sec.find("h3", class_="ProjectDescription__Title-sc-r2aril-1")
        if not title_el:
            continue
        heading = title_el.get_text(strip=True)
        content_div = sec.find("div", class_="ProjectDescription__DescriptionBase-sc-r2aril-8")
        content = content_div.get_text(strip=True) if content_div else ""
        if heading == "ãªã«ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹":
            what_text = content
        elif heading == "ãªãœã‚„ã‚‹ã®ã‹":
            why_text = content
        elif heading == "ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹":
            how_text = content
        elif heading == "ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™":
            do_text = content

    members = []
    member_sections = soup.find_all("section", class_="ProjectMemberListLaptop__Section-sc-11asydj-15")
    for ms in member_sections:
        name_tags = ms.find_all("p", class_="ProjectMemberListLaptop__ProjectMemberName-sc-11asydj-13")
        for nt in name_tags:
            member_name = nt.get_text(strip=True)
            members.append(member_name)

    stories = []
    heading_elems = soup.find_all("h3")
    for heading in heading_elems:
        if heading.get_text(strip=True) == "ä¼šç¤¾ã®æ³¨ç›®ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼":
            parent_section = heading.find_parent("section")
            if not parent_section:
                continue
            story_blocks = parent_section.find_all("div", class_="PinnedStoryList__Base-sc-w8czck-5")
            for block in story_blocks:
                title_box = block.find("h3", class_="PinnedStoryList__PostTitleBase-sc-w8czck-10")
                if not title_box:
                    continue
                link_tag = title_box.find("a")
                if not link_tag:
                    continue
                story_title = link_tag.get_text(strip=True)
                story_url = link_tag.get("href", "")
                stories.append({
                    "title": story_title,
                    "url": story_url,
                })

    return {
        "id": extract_job_id_from_url(url),
        "url": url,  # URLã‚’è¿½åŠ 
        "main_title": main_title,
        "company_name": company_name,
        "what": what_text,
        "why":  why_text,
        "how":  how_text,
        "do":   do_text,
        "members": members,
        "stories": stories,
    }

# Embeddingå–å¾—é–¢æ•°
def get_embedding(text, model="text-embedding-3-small"):
    # Call the embeddings endpoint via the client
    response = client.embeddings.create(
        input=text,
        model=model
    )
    return response.data[0].embedding

# ã‚¿ã‚°ç”Ÿæˆé–¢æ•°
def generate_tags(job_data):
    prompt = f"""
    ä»¥ä¸‹ã®æ±‚äººæƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¿ã‚°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:
    - ä¸»ãªå†…å®¹: {job_data['what']}
    - ãªãœã‚„ã‚‹ã®ã‹: {job_data['why']}
    - ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['how']}
    - ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™: {job_data['do']}

    å‡ºåŠ›ä¾‹: ["Python", "ãƒªãƒ¢ãƒ¼ãƒˆ", "ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ãƒ•ãƒ«ã‚¿ã‚¤ãƒ "]
    ã‚¿ã‚°ã¯JSONå½¢å¼ã®é…åˆ—ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    """

    # Using the client to create ChatCompletion
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=8000,
        temperature=0.7
    )

    result_text = response.choices[0].message.content.strip()

    try:
        tags = json.loads(result_text)
        if isinstance(tags, list):
            return tags
        else:
            return [tags]
    except json.JSONDecodeError:
        return [result_text]

# ä¸€æ„ã®Llama IDç”Ÿæˆé–¢æ•°
def generate_llama_id():
    return str(uuid.uuid4())

# SQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–¢æ•°        
def save_to_sql_db(job_data, sql_cursor):
    try:
        # LlamaIndexç”¨ã®ä¸€æ„IDã‚’ç”Ÿæˆ
        llama_id = generate_llama_id()

        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: ã‚¿ã‚°ã®ç¢ºèª
        print(f"Generated tags: {job_data['tags']}")

        # SQLã«ä¿å­˜
        sql_cursor.execute("""
            INSERT INTO job_metadata (
                id, llama_id, main_title, company_name, url, what, why, how, "do",
                members, stories, tags, is_active, created_at
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s,
                %s, %s, %s, %s, NOW()
            )
        """, (
            job_data["id"],          # ã‚¸ãƒ§ãƒ–ID
            llama_id,                # LlamaIndexã®ä¸€æ„ID
            job_data["main_title"],  # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
            job_data["company_name"],# ä¼šç¤¾å
            job_data["url"],         # URL
            job_data["what"],        # ãªã«ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹
            job_data["why"],         # ãªãœã‚„ã‚‹ã®ã‹
            job_data["how"],         # ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹
            job_data["do"],          # ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™
            json.dumps(job_data["members"]),  # ãƒ¡ãƒ³ãƒãƒ¼ (JSONå½¢å¼)
            json.dumps(job_data["stories"]),  # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ (JSONå½¢å¼)
            json.dumps(job_data["tags"]),     # ã‚¿ã‚° (JSONå½¢å¼)
            job_data.get("is_active", True)   # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
        ))

        # ã‚³ãƒŸãƒƒãƒˆã‚’è¿½åŠ 
        sql_cursor.connection.commit()
        print("SQL DB save completed.")

        return llama_id
    except Exception as e:
        print(f"Error saving to SQL DB: {e}")
        raise
        
# Upstashã«Vectorãƒ‡ãƒ¼ã‚¿ã¨Metaãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜   
def save_to_llama_index_with_embedding(job_data, index, sql_cursor):
    try:
        # SQLDBã«ä¿å­˜ã—ã€ä¸€æ„ã®Llama IDã‚’å–å¾—
        llama_id = save_to_sql_db(job_data, sql_cursor)
        print(f"Llama ID generated: {llama_id}")

        # çµåˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        combined_text = f"""
        {job_data['main_title']}
        {job_data['company_name']}
        ãªã«ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['what']}
        ãªãœã‚„ã‚‹ã®ã‹: {job_data['why']}
        ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹: {job_data['how']}
        ã“ã‚“ãªã“ã¨ã‚„ã‚Šã¾ã™: {job_data['do']}
        """
        print("Combined text for embedding:", combined_text)

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embedding = get_embedding(combined_text, model="text-embedding-3-small")
        print("Generated embedding:", embedding)

        # ã‚¿ãƒ—ãƒ«å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿
        file_id = str(uuid.uuid4())  # UUID4ã§ä¸€æ„ã®IDã‚’ç”Ÿæˆ
        metadata = {
            "llama_id": llama_id,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        print("Metadata for Upstash:", metadata)

        # Upstash Vectorã«ä¿å­˜
        index.upsert(vectors=[(file_id, embedding, metadata)])
        print("Upstash save completed.")

        # çµæœã‚’è¾æ›¸å½¢å¼ã§è¿”ã™
        return {
            "llama_id": llama_id,
            "file_id": file_id,
            "status": "success",
            "message": "Data successfully saved to SQL and Upstash"
        }

    except Exception as e:
        print(f"Error saving to Upstash: {e}")
        raise

# --- Streamlit UI Flow ---
if st.button("Fetch Job Data"):
    # 1. å…¥åŠ›æ¤œè¨¼
    if not job_url:
        st.error("Please enter a URL.")
    elif not job_id:
        st.error("Please enter a valid Job ID.")
    else:
        try:
            # 2. Wantedlyã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
            job_data = fetch_wantedly_job_details(job_url)
            
            # 3. LLMã‚’ä½¿ã£ãŸã‚¿ã‚°ç”Ÿæˆ
            tags = generate_tags(job_data)
            job_data['tags'] = tags  # Ensure tags are assigned to job_data

            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: ã‚¿ã‚°ã®ç¢ºèª
            print(f"Generated tags: {tags}")

            # 4. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ï¼ˆä¸€æ™‚ä¿å­˜ï¼‰
            st.session_state["job_data"] = job_data
            st.session_state["tags"] = tags
            st.session_state["job_id"] = job_id

            # 5. ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆHITLãƒ—ãƒ­ã‚»ã‚¹ï¼‰
            st.subheader("Fetched Job Data (Preview)")
            st.write("### Main Title:", job_data["main_title"])
            st.write("### Company Name:", job_data["company_name"])
            st.write("### What:", job_data["what"])
            st.write("### Why:", job_data["why"])
            st.write("### How:", job_data["how"])
            st.write("### Do:", job_data["do"])
            st.write("### Members:", job_data["members"])
            st.write("### Stories:", job_data["stories"])
            st.write("### Tags (Generated):", tags)

            st.info("Check the preview above. Then click 'Approve and Save' if it looks good.")
        except Exception as e:
            # ä¾‹å¤–å‡¦ç†
            st.error(f"Error fetching job data: {e}")

# 6. ä¿å­˜ï¼ˆHITLã®æ‰¿èªå¾Œã«å®Ÿè¡Œï¼‰
if st.session_state["job_data"] and st.session_state["job_id"] and st.button("Approve and Save"):
    try:
        # LlamaIndexã¨Upstash Vectorã¸ã®ä¿å­˜
        result = save_to_llama_index_with_embedding(st.session_state["job_data"], vector_index, cursor)
        st.success(
            f"Job '{st.session_state['job_data']['main_title']}' "
            f"with ID '{st.session_state['job_id']}' "
            "saved successfully to UpStash and SQL!"
        )
        st.write("Save result:", result)
    except Exception as e:
        # ä¾‹å¤–å‡¦ç†
        st.error(f"Error saving job data: {e}")
